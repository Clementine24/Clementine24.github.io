<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>笨蛋启示录</title>
      <link href="/2022/03/28/%E7%AC%A8%E8%9B%8B%E5%90%AF%E7%A4%BA%E5%BD%95/"/>
      <url>/2022/03/28/%E7%AC%A8%E8%9B%8B%E5%90%AF%E7%A4%BA%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>这篇博客主要记载一些编程时遇到的比较偏、怪的问题，大多数很难在网络上搜寻到，是多次尝试后的结果。为日后查询提供参考。</p><span id="more"></span><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><ul><li><p>Numpy数组有一个独特的优势在于可以使用隐式的tuple数据类型作为索引，MATLAB和一般的Python数组都无法做到。</p></li><li><p>pip安装“ProxyError”，在命令行输入</p><p>unset HTTP_PROXY</p><p>unset HTTPS_PROXY</p><p>unset ALL_PROXY</p><p>unset all_PROXY</p><p>重新pip即可</p><p>如果还不行，输入set，一条条看还有没有https_proxy=127.0.0.1:1080这种，有就继续上面的命令</p></li><li><p>nn.BatchNorm1d()的输入参数num_features的取值：</p><p>输入维度是(N, C, L)时，num_features应该取C；这里N是batch size，C是数据的channel，L是数据长度。</p><p>输入维度是(N, L)时，num_features应该取L；这里N是batch size，L是数据长度，这时可以认为每条数据只有一个channel，省略了C</p><p>nn.BatchNorm2d()的输入参数num_features的取值为channel数</p></li></ul><h2 id="MATLAB"><a href="#MATLAB" class="headerlink" title="MATLAB"></a>MATLAB</h2><ul><li>max（）返回最大值，但max（array，[]，dim）返回所求dim最大索引下标。</li><li>reshape函数和array（：）以及array（a:b）=matrix解锁自成一派，建议不要自己索引打乱它。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Programming </tag>
            
            <tag> Guide </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning: Introduction</title>
      <link href="/2022/02/08/Deep-Learning-Introduction/"/>
      <url>/2022/02/08/Deep-Learning-Introduction/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>深度学习三步骤：定义网络、损失函数、优化</p><span id="more"></span><h2 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h2><h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>全连接层</p><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>全连接的前馈神经网络参数较多，需要较多样本学习；且很难提取局部不变性特征（过拟合）。</p><p>引入卷积神经网络，也是一种前馈神经网络，结构特性：局部连接、权重共享、空间或时间上的次采样</p><h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p>常用于计算信号的延迟积累</p><p>时刻t收到的信号$y_t$（卷积输出）为当前时刻产生的信息和以前时刻延迟的信息（输入信号序列）叠加：</p><script type="math/tex; mode=display">y_t=\sum_{k=1}^Kw_kx_{t-k+1}</script><p>$w_k$称为滤波器或卷积核。</p><p>假设input信号序列长度为N，卷积核长度为K，则输出卷积信号长度为N-K+1.</p><h3 id="卷积作用"><a href="#卷积作用" class="headerlink" title="卷积作用"></a>卷积作用</h3><ul><li>近似微分：例：$[-\frac{1}{2},0,\frac{1}{2}]$近似一阶；$[1,-2,1]$近似二阶</li><li>低通滤波/高通滤波：例：$w=[\frac{1}{3},\frac{1}{3},\frac{1}{3}]$检测信号序列中的低频信息（变换缓慢，如均值）；$w=[1,-2,1]$检测信号序列中的高频信息（变化较快，如二阶导）</li></ul><h3 id="卷积扩展"><a href="#卷积扩展" class="headerlink" title="卷积扩展"></a>卷积扩展</h3><ul><li>滑动步长S：增加步长减少信号输出长度</li><li>零填充P：往信号输入两端补充P个0</li></ul><p>根据卷积结果长度可以分为三类：</p><ul><li>窄卷积：S=1，P=0.输出长度为M-K+1  <font color="green">早期文献默认</font></li><li>宽卷积：S=1，P=K-1.输出长度为M+K-1</li><li>等宽卷积：S=1，P=(K-1)/2.输出长度为M  <font color="green">目前文献默认</font></li></ul><h3 id="二维卷积"><a href="#二维卷积" class="headerlink" title="二维卷积"></a>二维卷积</h3><p>应用于图像处理等二维矩阵形式信息序列</p><p>定义一个输入信息$X$和滤波器$W$的二维卷积为$Y=W*X$，其中</p><script type="math/tex; mode=display">y_{ij}=\sum_{u=1}^U\sum_{v=1}^Vw_{uv}x_{i-u+1,j-v+1}</script><p>同样有步长S和零填充P供选择。</p><p>作用：作为特征提取器</p><p>应用到神经网络中，希望自动学习这个卷积核$W$，用以提取特征。</p><h3 id="神经网络-1"><a href="#神经网络-1" class="headerlink" title="神经网络"></a>神经网络</h3><p>用卷积层代替全连接层。</p><p>参数数量大大降低，只有卷积核数量个。</p><h4 id="互相关"><a href="#互相关" class="headerlink" title="互相关"></a>互相关</h4><p>计算卷积需要进行卷积核翻转，比较麻烦，但我们的目的其实是提取特征，因此翻转是不必要的。修改为互相关形式：</p><script type="math/tex; mode=display">y_{ij}=\sum_{u=1}^U\sum_{v=1}^Vw_{ij}x_{i+u-1,j+v-1}</script><p>之后除非特别说明，一般都为互相关。</p><h4 id="多个卷积核"><a href="#多个卷积核" class="headerlink" title="多个卷积核"></a>多个卷积核</h4><p>使用多个卷积核，得到多个特征映射(depth)，并将其拼接起来作为输出。</p><p>例如某卷积层：</p><ul><li>输入：D个特征映射$M\times N\times D$</li><li>输出：P个特征映射$M’\times N’\times P$</li></ul><h4 id="卷积层的映射关系"><a href="#卷积层的映射关系" class="headerlink" title="卷积层的映射关系"></a>卷积层的映射关系</h4><script type="math/tex; mode=display">Z^P=W^P\otimes X+b^P=\sum_{d=1}^DW^{P,d}\otimes X^d+b^P,\\Y^P=f(Z^P)</script><p>可以将P个输出的特征映射看为输入的D个特征映射的<strong><em>全连接</em></strong>！</p><p>卷积层结构：</p><p><img src="/2022/02/08/Deep-Learning-Introduction/卷积层结构.png" alt="卷积层结构"></p><h4 id="池化层（汇聚层）"><a href="#池化层（汇聚层）" class="headerlink" title="池化层（汇聚层）"></a>池化层（汇聚层）</h4><p>卷积层可以显著减少连接个数（前馈神经网络是每一个输入信号有一个单独权重，得到总加权和，卷积神经网络是只有少数权重滑动加权，得到多个加权和），但是每个特征映射的神经元个数并没有显著减少（M-K+1）。</p><p>因此使用汇聚，将大的特征映射划分为几个小的特征映射，从每个小块中提取出一个代表元素。如：最大汇聚、平均汇聚、max-in-time(取每个depth最大的元素，好处是控制输出维数，只和卷积核的数量相关）。</p><font color="green">可以看做特殊的卷积层</font><p>池化层结构：</p><p><img src="/2022/02/08/Deep-Learning-Introduction/汇聚层结构.png" alt="池化层结构"></p><h4 id="卷积网络结构"><a href="#卷积网络结构" class="headerlink" title="卷积网络结构"></a>卷积网络结构</h4><p><img src="/2022/02/08/Deep-Learning-Introduction/卷积网络结构.png" alt="卷积网络结构"></p><h3 id="其他卷积网络"><a href="#其他卷积网络" class="headerlink" title="其他卷积网络"></a>其他卷积网络</h3><h4 id="空洞卷积"><a href="#空洞卷积" class="headerlink" title="空洞卷积"></a>空洞卷积</h4><p>增加输出单元的感受野的方法：</p><ul><li>增加卷积核的大小</li><li>增加层数</li><li>在卷积之前进行汇聚操作（但会丢失部分信息）</li><li><strong>空洞卷积</strong>：通过给卷积增加“空洞”来变相地增加其大小</li></ul><p><img src="/2022/02/08/Deep-Learning-Introduction/空洞卷积.png" alt="空洞卷积"></p><h4 id="转置卷积-微步卷积"><a href="#转置卷积-微步卷积" class="headerlink" title="转置卷积/微步卷积"></a>转置卷积/微步卷积</h4><p>一般的卷积是高维映射为低维，该卷积将低维特征映射到高维特征。</p><p>思想就是将步长S调为小于1的值，方法为在元素之间以及矩阵周围插入适量的空格。</p><p><img src="/2022/02/08/Deep-Learning-Introduction/微步卷积.png" alt="微步卷积"></p><h4 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h4><p>问题：在深度网络中，使用一个非线性单元去逼近目标函数，会发现当目标函数为线性函数时，效果非常差。</p><p>解决方法：将目标函数拆分为两部分：恒等函数和禅茶函数：</p><script type="math/tex; mode=display">h(x)=\underbrace{x}_{恒等函数}+\underbrace{(h(x)-x)}_{残差函数}</script><p>给非线性的卷积层（$h(x)-x$）添加直连边(shortcut connection)的方式来提高信息的传播效率。</p><p><img src="/2022/02/08/Deep-Learning-Introduction/残差网络.png" alt="残差网络"></p><p>好处：当卷积函数的导数比较小时，加了一个x相当于加了偏置1，使得不会出现链式法则梯度消失的情况。</p><h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p>不同于前两种，该网络信息可以反向传递。</p><p>当信号序列与时间相关时，有时候t时刻的输出不仅和t时刻的输入相关，还会和前面时间的输入、输出相关，但在前馈网络中，假设每次输入都是独立的，也即每次网络的输出只依赖于当前的输入。<strong>如何给网络增加记忆能力，处理任意长度的时序数据？</strong>：</p><h3 id="时延神经网络"><a href="#时延神经网络" class="headerlink" title="时延神经网络"></a>时延神经网络</h3><p>建立额外的延时单元，用以储存网络的历史信息（输入、输出、隐状态……）</p><script type="math/tex; mode=display">h_t^{(l)}=f(h_{t}^{l-1},h_{t-1}^{l-1},\cdots,h_{t-K}^{l-1})</script><p><img src="/2022/02/08/Deep-Learning-Introduction/延时神经网络.png" alt="延时神经网络"></p><h3 id="有外部输入的非线性自回归模型"><a href="#有外部输入的非线性自回归模型" class="headerlink" title="有外部输入的非线性自回归模型"></a>有外部输入的非线性自回归模型</h3><p>自回归模型的加强版</p><p><strong>自回归模型</strong>：一类时间序列模型，$y_t=w_0+\sum_{k=1}^Kw_ky_{t-k}+\epsilon_t$，其中$\epsilon_t\sim N(0,\sigma^2)$为噪声</p><p>有外部输入的非线性自回归模型：</p><script type="math/tex; mode=display">y_t=f(x_t,x_{t-1},\cdots x_{t-K_x},y_{t-1},y_{t-2},\cdots,y_{t-K_y})</script><p>其中$f(\cdot)$是一个非线性函数 <font color="green">可以是前馈神经网络</font></p><p><img src="/2022/02/08/Deep-Learning-Introduction/非线性自回归.png" alt="非线性自回归"></p><h3 id="循环神经网络-1"><a href="#循环神经网络-1" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><p>使用带自反馈的神经元：$h_t=f(h_{t-1},x_t)$</p><p><img src="/2022/02/08/Deep-Learning-Introduction/循环神经网络.png" alt="循环神经网络"></p><h4 id="简单循环网络（SRN）"><a href="#简单循环网络（SRN）" class="headerlink" title="简单循环网络（SRN）"></a>简单循环网络（SRN）</h4><script type="math/tex; mode=display">h_t=f(Uh_{t-1}+Wx_t+b)</script><p>由<strong>循环神经网络的通用近似定理</strong>可知，一个完全连接的循环网络是任何非线性动力系统的近似器。</p><p><strong>图灵完备</strong>：所有的图灵机都可以被一个由使用Sigmoid型激活函数的神经元构成的全连接循环网络来进行模拟。</p><p>根据图灵完备，一个完全连接的循环神经网络可以近似解决所有可计算问题。</p><h4 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h4><p>以SRN为例，记$L=\sum_{i=1}^TL_t$为损失函数，则有</p><script type="math/tex; mode=display">z_t=Uh_{t-1}+Wx_{t}+b\\h_t=f(z_t)</script><p>根据以上两条公式，可推得（注意，基于寻欢神经网络的特性，$z_t$不止和$h_{t-1}$有关，事实上，是和所有$t-1$之前的$h$有关，求导时要注意）：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial U}=\sum_{t=1}^T\frac{\partial L_t}{\partial U}=\sum_{t=1}^T\sum_{k=1}^t\frac{\partial L_t}{\partial z_k}h_{t-1}^T=\sum_{t=1}^T\sum_{k=1}^t\delta_{t,k}h_{t-1}^T</script><p>P47 7:00</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>test</title>
      <link href="/2022/01/29/test/"/>
      <url>/2022/01/29/test/</url>
      
        <content type="html"><![CDATA[<h1 id="Test-text"><a href="#Test-text" class="headerlink" title="Test text"></a>Test text</h1><p>Something for test hexo and blog and markdown.</p><p>测试用文档</p><span id="more"></span><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;hello Clementine&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="Photo"><a href="#Photo" class="headerlink" title="Photo"></a>Photo</h2><p><img src="/2022/01/29/test/rose.jpg" alt="rose"></p><h2 id="Link"><a href="#Link" class="headerlink" title="Link"></a>Link</h2><p><a href="https://github.com/clementine24">My Github</a></p>]]></content>
      
      
      <categories>
          
          <category> test </category>
          
      </categories>
      
      
        <tags>
            
            <tag> test1 </tag>
            
            <tag> test2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/01/26/hello-world/"/>
      <url>/2022/01/26/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><span id="more"></span><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
